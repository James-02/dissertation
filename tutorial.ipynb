{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2996cf",
   "metadata": {},
   "source": [
    "# Reservoir Computing with Genetic Oscillators for Arrhythmia Classification Tutorial\n",
    "\n",
    "In this notebook, we will guide you through a comprehensive tutorial on utilizing our genetic oscillator-based reservoir for reservoir computing to classify ECG signals into 5 different categories of arrhythmia. It provides a deep dive into the implementation and usage of our innovative approach.\n",
    "\n",
    "Please note that due to the computational complexity of certain sections, we will be loading results from previous runs stored in local storage. These stored results are not available as part of the GitHub repository but are essential for reproducing our experiments and understanding the tutorial.\n",
    "\n",
    "Throughout the tutorial, we will also incorporate various visualization techniques to facilitate a better understanding of the internal workings of our reservoir and the dataset.\n",
    "\n",
    "Now lets explore the potential of reservoir computing for arrhythmia classification using genetic oscillators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97df15",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6ec72",
   "metadata": {},
   "source": [
    "### 1.1. Planting Seeds\n",
    "To ensure reproducibility of our experiments, we fix the random state of our system with a static seed. This guarantees that the same results will be obtained each time the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab7b04",
   "metadata": {
    "metadata": {},
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import reservoirpy as rpy\n",
    "\n",
    "# Define global seed for random state generators\n",
    "SEED = 1234\n",
    "\n",
    "# Set reservoirpy verbosity to 1 (enabled)\n",
    "VERBOSITY = 0\n",
    "\n",
    "# Fix our random state generators\n",
    "rpy.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# set reservoirpy verbosity to 0 (OFF)\n",
    "rpy.verbosity(VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe9633",
   "metadata": {},
   "source": [
    "### 1.2. Initializing the Logger\n",
    "\n",
    "In this section, we establish logging to capture detailed information throughout our tutorial. We import a custom `Logger` class from `utils.logger`, designed to streamline logging operations.\n",
    "\n",
    "Our `Logger` class, built upon Python's `logging` module, enables parallel writes to both stream and file while maintaining a single handler per type. This setup ensures efficient tracking of hyperparameters and metrics, vital for effective model evaluation and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0c7e2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.logger import Logger\n",
    "\n",
    "# Set our logging level to 1 (DEBUG)\n",
    "LOG_LEVEL = 1\n",
    "\n",
    "# Initialize our logger with a log file\n",
    "log_name = \"reservoir-computing-tutorial\"\n",
    "log_file = f\"logs/{log_name}.log\"\n",
    "logger = Logger(level=LOG_LEVEL, log_file=log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386f27d",
   "metadata": {},
   "source": [
    "## 2. Visualizing ECGs\n",
    "\n",
    "This section focuses on visualizing an electrocardiogram (ECG) waveform, including a sampled segment, to visualize our dataset\n",
    "\n",
    "We begin by loading our forecasting ECG data using a function from our `utils.preprocessing` module, setting the number of timesteps to `2000`. The function returns the ECG data as `X_forecast`.\n",
    "\n",
    "Notably, we will not be using this dataset for classification, we will be using segments of it, but it is perfect for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba178e07",
   "metadata": {},
   "source": [
    "### 2.1. Loading a Full ECG Signal\n",
    "Here we load an ECG signal of 2000 timesteps from our forecasting dataset.\n",
    "\n",
    "As mentioned, we will be using 187 timestep, labeled, segments of this dataset for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965f3d3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.preprocessing import load_ecg_forecast\n",
    "\n",
    "timesteps = 2000\n",
    "\n",
    "# Load only the X_train subset as we just want to visualise the data\n",
    "X_forecast, _, _, _ = load_ecg_forecast(timesteps=timesteps)\n",
    "\n",
    "print(f\"Forecasting Data Shape: \", X_forecast.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e27291",
   "metadata": {},
   "source": [
    "### 2.2. Visualizing an ECG\n",
    "In this section, we plot the ECG signal over time (in milliseconds), calculated using a sampling frequency of 125Hz.\n",
    "\n",
    "Additionally, we draw a box around a segment of 187 timesteps from the ECG signal, representing a single instance of our classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225b645",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sampling_freq = 125\n",
    "num_samples = len(X_forecast)\n",
    "time_ms = np.arange(num_samples) / sampling_freq * 1000\n",
    "\n",
    "# Plot the ECG waveform\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(time_ms, X_forecast, label='ECG waveform')\n",
    "\n",
    "# Calculate box width in ms\n",
    "box_width = 187 * 1000 / sampling_freq\n",
    "start_x = 4500\n",
    "\n",
    "# Plot the box\n",
    "plt.plot([start_x, start_x + box_width], [min(X_forecast), min(X_forecast)], linestyle=':', color='red', label=\"Sampled Segment\")\n",
    "plt.plot([start_x, start_x + box_width], [max(X_forecast), max(X_forecast)], linestyle=':', color='red')\n",
    "plt.plot([start_x, start_x], [min(X_forecast), max(X_forecast)], linestyle=':', color='red')\n",
    "plt.plot([start_x + box_width, start_x + box_width], [min(X_forecast), max(X_forecast)], linestyle=':', color='red')\n",
    "\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Voltage (mV)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4b8b9",
   "metadata": {},
   "source": [
    "## 3. Loading our Classification Dataset\n",
    "\n",
    "We begin by loading our ECG dataset using a function from our preprocessing module. This function provides flexibility with several parameters:\n",
    "\n",
    "- `rows`: Specifies the number of instances to load. To maintain class balance, it's capped at 4015.\n",
    "- `test_ratio`: Determines the portion of the dataset allocated to the test subset. We'll use 0.2 to reserve 20% for testing.\n",
    "- `encode_labels`: Ensures that categorical labels are one-hot encoded, necessary for reservoirpy.\n",
    "- `binary`: Indicates whether to merge all arrhythmia classes into one for binary classification.\n",
    "- `noise_rate`: Determines the rate of noise applied to augmented instances, ranging between [0-1]\n",
    "- `noise_ratio`: Determines the ratio of instances within the dataset to augment by percentage e.g. 0.2 == 20%\n",
    "\n",
    "Let's utilize these parameters to load our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048cb16",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.preprocessing import load_ecg_data\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_ecg_data(\n",
    "    rows=4015,\n",
    "    test_ratio=0.2,\n",
    "    encode_labels=True,\n",
    "    binary=False,\n",
    "    noise_rate=0.2,\n",
    "    noise_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681f49c",
   "metadata": {},
   "source": [
    "## 4. Visualizing our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e12abc",
   "metadata": {},
   "source": [
    "### 4.1. Concatenating Training and Test Subsets\n",
    "\n",
    "Before visualization, we concatenate the training and test subsets of our dataset into a single dataset, of targets `X` and labels `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b970441",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# concatenate the training and test subsets into one\n",
    "import numpy as np\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "Y = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2bd75",
   "metadata": {},
   "source": [
    "### 4.2. Plotting Dataset Distribution\n",
    "We use the `plot_data_distribution` function to visualize the distribution of classes in our dataset and save the plot as \"data-distribution.png\".\n",
    "\n",
    "Notably, we visualize the class balancing performed by our preprocessing module here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64fbf3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_data_distribution\n",
    "\n",
    "plot_data_distribution(Y, filename=\"data-distribution.png\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d588b",
   "metadata": {},
   "source": [
    "### 4.3. Plotting Dataset Information\n",
    "Next, we use the plot_dataset_info function to visualize various statistics and information about our dataset, such as the total instances, class size, and the shape of our targets and labels respectively, which have been reshaped for reservoir computing classification input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_dataset_info\n",
    "\n",
    "plot_dataset_info(X_train, Y_train, X_test, Y_test, filename=\"dataset-info.png\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099540de",
   "metadata": {},
   "source": [
    "### 4.4. Plotting Standard Deviation and Mean Across Classes\n",
    "We further visualize the class standard deviation, and class mean using the corresponding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02042e95",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_class_std\n",
    "from utils.visualisation import plot_class_mean\n",
    "\n",
    "plot_class_std(X, Y, filename=\"class-std.png\", show=True)\n",
    "plot_class_mean(X, Y, filename=\"class-means.png\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a8f33",
   "metadata": {},
   "source": [
    "### 4.5. Visualising Class Averages\n",
    "Next, we can visualise the average of each class with a single representative waveform which represents the mean of each time point within each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd9f57",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_average_instance\n",
    "\n",
    "plot_average_instance(X, Y, filename=\"average-instance.png\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04009736",
   "metadata": {},
   "source": [
    "# 5. Visualising Genetic Oscillators\n",
    "In order to understand how our reservoir will operate, we can use a sine wave to test the forward function of a single Oscillator node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f944a1",
   "metadata": {},
   "source": [
    "### 5.1. Genetic Oscillators in Action\n",
    "We can simulate the behaviour of gene expression using our Oscillator node by feeding in a continuous input of a sine wave over a period of 1000 timesteps.\n",
    "\n",
    "Internally, our genetic oscillators are composed of a system of delay differential equations representing the interactions between the coupled genes luxI (I) and aiiA (A), commonly found in bacteria. luxI produces a quorum sensing molecule called AHL (Hi), which diffuses outside of the cell to, binding to the other cell's luxI promoters (He).\n",
    "\n",
    "For each call of our Oscillator node's `forward` function, we receive an array of system states of shape (1, 4), where we have a single state for each variable in the system (A, I, Hi, He) at the current time point.\n",
    "\n",
    "- `A` represents the aiiA gene, whilst `I` represents the luxI gene of the system.\n",
    "\n",
    "- `Hi` and `He`, represent the internal and external AHL quorum-sensing molecules respectively.\n",
    "\n",
    "We also apply a `coupling` coefficient to our sine wave input at each time step, representing the cell-to-cell coupling strength of cells within an engineered genetic circuit.\n",
    "\n",
    "We can collect these states over the time series to represent the complete state of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb18c84",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from reservoir.node import Oscillator\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 1000\n",
    "coupling = 1e-5\n",
    "warmup = 100\n",
    "\n",
    "timespan = np.linspace(0, timesteps, timesteps)\n",
    "sine_wave = np.sin(timespan)\n",
    "\n",
    "oscillator = Oscillator(timesteps=timesteps, warmup=warmup)\n",
    "\n",
    "states = []\n",
    "for state in oscillator.warmup_states:\n",
    "    states.append(state)\n",
    "\n",
    "for i in range(timesteps):\n",
    "    states.append(oscillator.forward(sine_wave[i] * coupling))\n",
    "\n",
    "states = np.array(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f671bab",
   "metadata": {},
   "source": [
    "### 5.2. Visualising Genetic Oscillator States\n",
    "We can now visualise the states of the coupled genes (A, I), as well as the internal and external signals (Hi, He) over the time period.\n",
    "\n",
    "We can see the synchronized coupled oscillations by genes A and I, where the delayed He signal drives the production of the I gene promoter.\n",
    "\n",
    "Notably, we can visualize the warmup process here, in which we run the system with no input for a number of time steps to initially promote gene concentrations, and save the states produced as the default permanent history of the node, preventing the need for warming up during each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b414953",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Oscillator States Shape: \", states.shape)\n",
    "\n",
    "timesteps = len(states)\n",
    "time = np.linspace(0, timesteps, timesteps)\n",
    "labels=[\"aiiA (A)\", \"luxI (I)\", \"Internal AHL (Hi)\", \"External AHL (He)\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(0, states.shape[1]):\n",
    "    plt.plot(time[warmup:], states[warmup:, i], label=labels[i] if labels else None, color='C'+str(i))\n",
    "\n",
    "# Plot warmup separately to ensure shared label in legend\n",
    "for i in range(0, states.shape[1]):\n",
    "    plt.plot(time[:warmup], states[:warmup, i], linestyle=\"dotted\", color='C'+str(i))\n",
    "\n",
    "# Add only one label for the warmup phase\n",
    "plt.plot([], [], linestyle=\"dotted\", label=\"Warmup\", color='grey')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Concentration\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296103a4",
   "metadata": {},
   "source": [
    "## 6. Constructing an Echo State Network\n",
    "Here we will initialize our reservoir and readout nodes, the two key layers within an echo-state network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4da80",
   "metadata": {},
   "source": [
    "### 6.1. Initializing the Reservoir\n",
    "To initialize our reservoir we must supply a range of important hyperparameters:\n",
    "- `timesteps` the timespan of each data instance, allows the Oscillator nodes to know when to reset their internal history.\n",
    "- `nodes` the number of Oscillator nodes within the reservoir.\n",
    "- `delay` the time delay of the external He signal within the genetic oscillators.\n",
    "\n",
    "We construct a dictionary using the `delay` parameter to pass to the `OscillatorReservoir` as the `node_kwargs` parameter. The reservoir will propagate the parameters directly through to the Oscillator nodes as `**kwargs`.\n",
    "\n",
    "Note, there are further hyper-parameters of the reservoir but for now we will focus on these as they are integral to our system producing oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871c89f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from reservoir.reservoir import OscillatorReservoir\n",
    "\n",
    "timesteps = X_train[0].shape[0]\n",
    "print(\"Timesteps: \", timesteps)\n",
    "\n",
    "nodes = 250\n",
    "delay = 10\n",
    "\n",
    "node_kwargs = {'delay': delay}\n",
    "\n",
    "reservoir = OscillatorReservoir(\n",
    "    units=nodes,\n",
    "    timesteps=timesteps,\n",
    "    node_kwargs=node_kwargs,\n",
    "    seed=SEED,\n",
    "    name=\"OscillatorReservoir\")\n",
    "\n",
    "print(\"Reservoir Hyperparameters: \", reservoir.hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3e9c9",
   "metadata": {},
   "source": [
    "### 6.2. Initializing our Readout Node\n",
    "We will be using a reservoirpy `Ridge` node as our readout node which employs a layer of `N` neurons, where `N` is the number of classes in our task, each Ridge neuron uses regularized Tikhonov linear regression to perform learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940ec99",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from reservoirpy.nodes import Ridge\n",
    "\n",
    "ridge = 1e-5\n",
    "readout = Ridge(ridge=1e-5)\n",
    "\n",
    "print(\"Ridge Hyperparameters: \", readout.hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3c280",
   "metadata": {},
   "source": [
    "### 6.3. Visualising Reservoir States\n",
    "We can visualise the activation of our reservoir by plotting the states produced by our reservoir for the timeseries of a single instance of data.\n",
    "\n",
    "Each line represents the luxI gene concentrations over a single instance's time series, for each node in the reservoir.\n",
    "\n",
    "Here we can visualize the complex dynamics of each genetic oscillator node, allowing us to transform a one-dimensional input, into a matrix of shape `(timesteps, nodes)`, giving us a high-dimensional representation for each instance.\n",
    "\n",
    "We can then map the high-dimensional representation of each instance to its true label during training.\n",
    "\n",
    "Notably, the high range of phase differences between wave forms is representative of reservoir's environment, as each node within the reservoir has specific input and recurrent connectivity, affecting its weight, alongside further hyperparameters such as bias, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af6bd3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_states\n",
    "\n",
    "states = reservoir.run(X_train[0])\n",
    "print(\"Reservoir States Shape: \", states.shape)\n",
    "\n",
    "plot_states(states, legend=False)  # disable legend to not pollute the figure with node labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae00b42",
   "metadata": {},
   "source": [
    "## 7. Classification Optimization\n",
    "\n",
    "With our classification dataset loaded and our echo state network (ESN) layers constructed, we're ready to perform classification.\n",
    "\n",
    "However, we don't know what the best system parameters are, and so, using a high-performing computing cluster, we perform a grid search of system parameters, aiming to maximize the F1 score of the classification of arrhythmia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e2ebd",
   "metadata": {},
   "source": [
    "### 7.1. Classification Objective\n",
    "Here we define the objective for our hyperparameter optimization task, to do this we:\n",
    "- Load our ECG classification dataset\n",
    "- Initialize our `OscillatorReservoir` with the current trial's hyperparameters\n",
    "- Initialize `Ridge` readout node\n",
    "- Perform classification across 5 folds of cross validation\n",
    "    - Notably, we add our data augmentation within each fold within the `classify` method.\n",
    "- Report the F1 score for this setup.\n",
    "\n",
    "As noted to ensure robustness, we utilize cross-validation to average our model performance metrics over k-folds. Additionally, we leverage multi-processing, allocating each fold to a new CPU core. This setup allows each fold to run independently with its copies of the reservoir and readout nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classification import classify\n",
    "from utils.preprocessing import load_ecg_data\n",
    "from reservoir.reservoir import OscillatorReservoir\n",
    "\n",
    "from reservoirpy.nodes import Ridge\n",
    "\n",
    "def objective(trial, **kwargs):\n",
    "    X_train, Y_train, X_test, Y_test = load_ecg_data(\n",
    "        rows=kwargs['instances'],\n",
    "        test_ratio=kwargs['test_ratio'],\n",
    "        binary=kwargs['binary']\n",
    "    )\n",
    "\n",
    "    noise_ratio = trial.suggest_float(\"noise_ratio\", 0, 1)\n",
    "    noise_rate = trial.suggest_float(\"noise_rate\", 0, 1)\n",
    "\n",
    "    reservoir = OscillatorReservoir(\n",
    "        units=trial.suggest_float(\"nodes\", 0, 300),\n",
    "        timesteps=X_train[0].shape[0],\n",
    "        sr=trial.suggest_float(\"sr\", 0, 2),\n",
    "        warmup=trial.suggest_float(\"warmup\", 0, 100),\n",
    "        coupling=trial.suggest_float(\"coupling\", 0, 1),\n",
    "        rc_scaling=trial.suggest_float(\"rc_scaling\", 0, 1),\n",
    "        input_connectivity=trial.suggest_float(\"input_connectivity\", 0, 1),\n",
    "        rc_connectivity=trial.suggest_float(\"rc_connectivity\", 0, 1),\n",
    "        input_scaling=trial.suggest_float(\"input_scaling\", 0, 1),\n",
    "        bias_scaling=trial.suggest_float(\"bias_scaling\", 0, 1),\n",
    "        node_kwargs={'delay': kwargs['delay']},\n",
    "        seed=kwargs['seed'])\n",
    "\n",
    "    readout = Ridge(ridge=kwargs['ridge'])\n",
    "\n",
    "    filename = f\"results/runs/{kwargs['study_name']}-{trial.number}\"\n",
    "\n",
    "    metrics = classify(reservoir, readout, X_train, Y_train, X_test, Y_test, folds=kwargs['folds'],\n",
    "                       save_file=filename, noise_rate=noise_rate, noise_ratio=noise_ratio)\n",
    "\n",
    "    return metrics['f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77af37",
   "metadata": {},
   "source": [
    "### 7.2. Hyperparameter Optimization Research\n",
    "\n",
    "Next, we create an `Optuna` study, and run the `research` method to find the best parameter over 5 trials.\n",
    "\n",
    "Notably, we do not run the `research` method within this notebook due to the computationally intensive nature, instead we load an existing study from our local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ec731",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from optimization.base import research\n",
    "\n",
    "# Define the grid of parameter values as a dictionary\n",
    "param_values = {\n",
    "    'sr': [0.9, 1.0, 1.1],\n",
    "    'nodes': [200, 225, 250],\n",
    "    'input_connectivity': [0.1, 0.3, 0.5],\n",
    "    'rc_connectivity': [0.1, 0.3, 0.5],\n",
    "    'coupling': [1e-3, 5e-2, 5e-3],\n",
    "    'warmup': [40, 50, 60],\n",
    "    'rc_scaling': [4e-6, 8e-6],\n",
    "    'input_scaling': [0.9, 1.0, 1.1],\n",
    "    'bias_scaling': [0.9, 1.0, 1.1],\n",
    "    'noise_ratio': [0.1, 0.2, 0.3],\n",
    "    'noise_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Create a list of parameter sets as dictionaries\n",
    "study_params = {\n",
    "    'delay': 10,\n",
    "    'seed': 1337,\n",
    "    'ridge': 1e-5,\n",
    "    'instances': 4015,\n",
    "    'binary': False,\n",
    "    'test_ratio': 0.2,\n",
    "    'folds': 5,\n",
    "    'study_name': \"classification\"\n",
    "}\n",
    "\n",
    "processes = 1\n",
    "trials = 5\n",
    "\n",
    "# Create the study with the GridSampler\n",
    "sampler = optuna.samplers.GridSampler(param_values)\n",
    "\n",
    "log_name = f\"logs/optuna-{study_params['study_name']}.db\"\n",
    "storage = optuna.storages.RDBStorage(f'sqlite:///{log_name}')\n",
    "study = optuna.create_study(\n",
    "    study_name=study_params['study_name'],\n",
    "    direction='maximize',\n",
    "    storage=storage,\n",
    "    sampler=sampler,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# perform objective research\n",
    "# research(study, trials, objective, processes, **study_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff50296",
   "metadata": {},
   "source": [
    "### 7.3. Evaluate Optimization Results\n",
    "\n",
    "Once optimization is complete, we can report the results, in which we discover the optimal set of system hyperparameters, alongside the resulting F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36107fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization.base import evaluate_study\n",
    "\n",
    "evaluate_study(study.trials_dataframe(), objective_str=\"F1 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3900df1",
   "metadata": {},
   "source": [
    "### 7.4. Plot Optimization Results\n",
    "\n",
    "We can plot the slice plot of our study's optimization trials here, depicting the F1 score of each trial, each with a unique set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization.base import plot_results\n",
    "\n",
    "from optuna.visualization.matplotlib import plot_slice\n",
    "\n",
    "plot_results(study, plot_slice, filename=\"classification-slice.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00cedcf",
   "metadata": {},
   "source": [
    "### 7.5. Plot Hyperparameter Importances\n",
    "\n",
    "We can explain the importance of each hyperparameter within the slice plot above using the following visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf336075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "plot_results(study, plot_param_importances, filename=\"classification-importances.png\")\n",
    "\n",
    "# reset matplotlib styling (optuna updates it globally)\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562bfe5",
   "metadata": {},
   "source": [
    "## 8. Readout Layer Optimization\n",
    "\n",
    "After we have discovered the optimal system hyperparameters, we can now optimize our readout node.\n",
    "\n",
    "During the previous optimization, we saved the important metadata to files, allowing us to utilize the states produced in order to train a range of different classifiers.\n",
    "\n",
    "We load the data which the `classify` function has just saved to our local `results/runs` directory which contains the performance metrics, model hyperparameters, and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16ef6d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.preprocessing import load_npz\n",
    "\n",
    "best_trial = 32\n",
    "\n",
    "filename = f\"results/runs/{study_params['study_name']}-{best_trial}\"\n",
    "\n",
    "folds_data = []\n",
    "\n",
    "folds = 5\n",
    "\n",
    "# extract each fold from the classification results\n",
    "for fold in range(folds):\n",
    "    fold_filename = filename + f\"-fold-{str(fold)}.npz\"\n",
    "    fold_data = load_npz(fold_filename, allow_pickle=True)\n",
    "    folds_data.append(fold_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e8b70",
   "metadata": {},
   "source": [
    "### 8.1. Extracting Trained States\n",
    "\n",
    "In order to fit and run our readout nodes, we require the previously trained reservoir states, for both our training and test sets, as well as their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72914570",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fold_states = {fold: {} for fold in range(folds)}\n",
    "\n",
    "# extract the trained states for each fold\n",
    "for fold in range(folds):\n",
    "    # Load trained states and matching labels\n",
    "    trained_states = folds_data[fold]['train_states']\n",
    "    tested_states = folds_data[fold]['test_states']\n",
    "    Y_train = folds_data[fold]['Y_train']\n",
    "    Y_test = folds_data[fold]['Y_test']\n",
    "\n",
    "    # Reshape arrays\n",
    "    trained_states = trained_states.reshape(trained_states.shape[0], 1, -1)\n",
    "    tested_states = tested_states.reshape(tested_states.shape[0], 1, -1)\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0], 1, -1)\n",
    "    Y_test = Y_test.reshape(Y_test.shape[0], 1, -1)\n",
    "\n",
    "    # Populate fold_states dictionary\n",
    "    fold_states[fold]['trained_states'] = trained_states\n",
    "    fold_states[fold]['tested_states'] = tested_states\n",
    "    fold_states[fold]['Y_train'] = Y_train\n",
    "    fold_states[fold]['Y_test'] = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55922a12",
   "metadata": {},
   "source": [
    "### 8.2. Analysing Different Classifiers\n",
    "Although we use Ridge as our default classifier, we can use a range of classifiers to analyse the differences, we define our `Optuna` optimization objective here.\n",
    "\n",
    "We perform a random sampling search over a range of hyperparameters for a range of different classifiers from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d773e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from reservoirpy.nodes import ScikitLearnNode\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from utils.classification import evaluate_performance\n",
    "\n",
    "def objective(trial, **kwargs):\n",
    "    # use the first fold to compare performance between classifiers\n",
    "    trained_states = fold_states[0]['trained_states']\n",
    "    tested_states = fold_states[0]['tested_states']\n",
    "    Y_train = fold_states[0]['Y_train']\n",
    "    Y_test = fold_states[0]['Y_test']\n",
    "\n",
    "    classifier_name = trial.suggest_categorical('classifier', kwargs['classifiers'])\n",
    "\n",
    "    if classifier_name == \"Ridge\":\n",
    "        hypers = {\n",
    "            \"alpha\": trial.suggest_float('ridge_alpha', 1e-5, 1e2, log=True),\n",
    "            \"fit_intercept\": trial.suggest_categorical('ridge_fit_intercept', [True, False]),\n",
    "            \"copy_X\": trial.suggest_categorical('ridge_copy_X', [True, False]),\n",
    "            \"tol\": trial.suggest_float('ridge_tol', 1e-5, 1e-1, log=True),\n",
    "            \"solver\": trial.suggest_categorical('ridge_solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']),\n",
    "        }\n",
    "        clf = RidgeClassifier\n",
    "\n",
    "    elif classifier_name == \"Bayes\":\n",
    "        hypers = {\n",
    "            \"var_smoothing\": trial.suggest_float('bayes_var_smoothing', 1e-12, 1e-4),\n",
    "        }\n",
    "        clf = GaussianNB\n",
    "\n",
    "    elif classifier_name == \"LR\":\n",
    "        hypers = {\n",
    "            \"tol\": trial.suggest_float('lr_tol', 1e-5, 1e-1, log=True),\n",
    "            \"C\": trial.suggest_float('lr_C', 1e-5, 1e5, log=True),\n",
    "            \"fit_intercept\": trial.suggest_categorical('lr_fit_intercept', [True, False]),\n",
    "            \"intercept_scaling\": trial.suggest_float('lr_intercept_scaling', 0.1, 10),\n",
    "            \"solver\": trial.suggest_categorical('lr_solver', ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']),\n",
    "            \"max_iter\": 1000,\n",
    "        }\n",
    "        clf = LogisticRegression\n",
    "\n",
    "    elif classifier_name == \"Perceptron\":\n",
    "        hypers = {\n",
    "            \"penalty\": trial.suggest_categorical('perceptron_penalty', [None, 'l2', 'l1', 'elasticnet']),\n",
    "            \"alpha\": trial.suggest_float('perceptron_alpha', 1e-5, 1e-2, log=True),\n",
    "            \"l1_ratio\": trial.suggest_float('perceptron_l1_ratio', 0, 1),\n",
    "            \"fit_intercept\": trial.suggest_categorical('perceptron_fit_intercept', [True, False]),\n",
    "            \"tol\": trial.suggest_float('perceptron_tol', 1e-5, 1e-1, log=True),\n",
    "            \"eta0\": trial.suggest_float('perceptron_eta0', 1e-4, 1, log=True),\n",
    "        }\n",
    "        clf = Perceptron\n",
    "\n",
    "    elif classifier_name == \"SVM\":\n",
    "        kernel = trial.suggest_categorical('svc_kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "        hypers = {\n",
    "            \"C\": trial.suggest_float('svc_C', 1e-5, 1e4, log=True),\n",
    "            \"kernel\": kernel,\n",
    "            \"degree\": trial.suggest_int('svc_degree', 1, 5),\n",
    "            \"gamma\": trial.suggest_categorical('svc_gamma', ['scale', 'auto']) if kernel != 'linear' else 'scale',\n",
    "            \"tol\": trial.suggest_float('svc_tol', 1e-5, 1e-2, log=True),\n",
    "        }\n",
    "        clf = SVC\n",
    "\n",
    "    elif classifier_name == \"MLP\":\n",
    "        hypers = {\n",
    "            \"hidden_layer_sizes\": trial.suggest_int('mlp_hidden_layer_sizes', 100, 300),\n",
    "            \"activation\": trial.suggest_categorical('mlp_activation', ['logistic', 'tanh', 'relu']),\n",
    "            \"solver\": trial.suggest_categorical('mlp_solver', ['lbfgs', 'adam']),\n",
    "            \"alpha\": trial.suggest_float('mlp_alpha', 1e-5, 1e-3, log=True),\n",
    "            \"learning_rate\": trial.suggest_categorical('mlp_learning_type', ['constant', 'invscaling']),\n",
    "            \"learning_rate_init\": trial.suggest_float('mlp_learning_rate_init', 1e-5, 1e-3, log=True),\n",
    "            \"power_t\": trial.suggest_float('mlp_power_t', 0.1, 1.0),\n",
    "            \"tol\": trial.suggest_float('mlp_tol', 1e-5, 1e-1, log=True),\n",
    "            \"momentum\": trial.suggest_float('mlp_momentum', 0.1, 0.9),\n",
    "            \"epsilon\": trial.suggest_float('mlp_epsilon', 1e-8, 1e-6, log=True),\n",
    "            \"max_iter\": 1000,\n",
    "        }\n",
    "        clf = MLPClassifier\n",
    "\n",
    "    elif classifier_name == \"KNN\":\n",
    "        hypers = {\n",
    "            \"n_neighbors\": trial.suggest_int('knn_n_neighbors', 1, 20),\n",
    "            \"weights\": trial.suggest_categorical('knn_weights', ['uniform', 'distance']),\n",
    "            \"algorithm\": trial.suggest_categorical('knn_algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "            \"leaf_size\": trial.suggest_int('knn_leaf_size', 10, 50),\n",
    "            \"p\": trial.suggest_float('knn_p', 1.0, 2.0)\n",
    "        }\n",
    "        clf = KNeighborsClassifier\n",
    "\n",
    "    elif classifier_name == \"DT\":\n",
    "        hypers = {\n",
    "            \"criterion\": trial.suggest_categorical('dt_criterion', [\"gini\", \"entropy\", \"log_loss\"]),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        }\n",
    "        clf = DecisionTreeClassifier\n",
    "\n",
    "    elif classifier_name == \"RF\":\n",
    "        hypers = {\n",
    "                \"n_estimators\": trial.suggest_int('rf_n_estimators', 100, 500),\n",
    "                \"criterion\": trial.suggest_categorical('rf_criterion', [\"gini\", \"entropy\", \"log_loss\"]),\n",
    "                \"oob_score\": trial.suggest_categorical('rf_oob_score', [True, False]),\n",
    "            }\n",
    "        clf = RandomForestClassifier\n",
    "\n",
    "    elif classifier_name == \"GB\":\n",
    "        hypers = {\n",
    "            \"loss\": trial.suggest_categorical('gb_loss', ['log_loss', 'exponential']),\n",
    "            \"learning_rate\": trial.suggest_float('gb_learning_rate', 0.001, 1.0),\n",
    "            \"n_estimators\": trial.suggest_int('gb_n_estimators', 100, 500),\n",
    "            \"subsample\": trial.suggest_float('gb_subsample', 0.1, 1.0),\n",
    "            \"criterion\": trial.suggest_categorical('gb_criterion', ['friedman_mse', 'squared_error']),\n",
    "        }\n",
    "        clf = GradientBoostingClassifier\n",
    "\n",
    "    node = ScikitLearnNode(clf, model_hypers=hypers)\n",
    "\n",
    "    logger.debug(f\"Fitting {classifier_name}\")\n",
    "    node.fit(trained_states, Y_train)\n",
    "\n",
    "    logger.debug(f\"Running {classifier_name}\")\n",
    "    Y_pred = node.run(tested_states)\n",
    "    return evaluate_performance(Y_pred, Y_test)['f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6747bd",
   "metadata": {},
   "source": [
    "### 8.3. Performing Readout Optimization\n",
    "\n",
    "We perform readout optimization in the same manor as the reservoir optimization, except we use random sampling over grid searching due to the much quicker run-time of the readout nodes, facilitating a much greater range of trials.\n",
    "\n",
    "We also do not perform this optimization within the notebook, and instead load previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c79b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "study_params = {\n",
    "    'classifiers': [\"Ridge\", \"Bayes\", \"LR\", \"Perceptron\", \"SVM\", \"MLP\", \"KNN\", \"DT\", \"RF\", \"GB\"],\n",
    "    'study_name': \"readout\"\n",
    "}\n",
    "\n",
    "db_name = f\"logs/optuna-{study_params['study_name']}.db\"\n",
    "trials = 5\n",
    "processes = 1\n",
    "\n",
    "storage = optuna.storages.RDBStorage(f'sqlite:///{db_name}')\n",
    "seed = np.random.randint(0, 10000)\n",
    "sampler = optuna.samplers.RandomSampler(seed=seed)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=study_params['study_name'],\n",
    "    direction='maximize',\n",
    "    storage=storage,\n",
    "    sampler=sampler,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# research(study, trials, objective, processes, **study_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c5f74",
   "metadata": {},
   "source": [
    "### 8.4. Evaluating Readout Optimization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda6873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from optimization.base import evaluate_study\n",
    "\n",
    "db_name = f\"logs/optuna-{study_params['study_name']}.db\"\n",
    "storage = optuna.storages.RDBStorage(f'sqlite:///{db_name}')\n",
    "\n",
    "study = optuna.load_study(study_name=study_params['study_name'], storage=storage)\n",
    "\n",
    "classifiers = [\"Ridge\", \"Bayes\", \"LR\", \"Perceptron\", \"SVM\", \"MLP\", \"KNN\", \"DT\", \"RF\", \"GB\"]\n",
    "\n",
    "# Run optimization for each classifier\n",
    "for classifier_name in classifiers:\n",
    "    study_df = study.trials_dataframe()\n",
    "    idx = study_df['params_classifier'] == classifier_name\n",
    "    classifier_study = study_df[idx]\n",
    "    evaluate_study(classifier_study, \"F1 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6057b9",
   "metadata": {},
   "source": [
    "## 9. Performing Optimized Classification\n",
    " \n",
    "Now that we have both of our results for reservoir and readout optimization, we must perform one final classification to concatenate these results across cross validation folds.\n",
    "\n",
    "Notably, the gradient boosting algorithm performed the best, however, due to its computationally intensive nature, we choose to continue with the K-Nearest Neighbors classifier, which followed closely behind in terms of performance.\n",
    "\n",
    "We use the best run from our K-Nearest Neighbors optimization study to construct a readout node with the optimized parameters, and perform prediction across the 5 cross-validation folds using our optimized reservoir states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from reservoirpy.nodes import ScikitLearnNode\n",
    "\n",
    "from utils.analysis import evaluate_performance\n",
    "\n",
    "classifier = KNeighborsClassifier\n",
    "name = \"K-Nearest Neighbors\"\n",
    "node_params = {\"n_neighbors\": 3, \"algorithm\": \"ball_tree\", \"leaf_size\": 14, \"p\": 1.5, \"weights\": \"uniform\"}\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, fold in enumerate(range(folds)):\n",
    "    print(f\"\\nCross-validation fold {idx}:\")\n",
    "    # Extract data for the current fold\n",
    "    trained_states = fold_states[fold]['trained_states']\n",
    "    tested_states = fold_states[fold]['tested_states']\n",
    "    Y_train = fold_states[fold]['Y_train']\n",
    "    Y_test = fold_states[fold]['Y_test']\n",
    "\n",
    "    # Initialize ScikitLearnNode\n",
    "    node = ScikitLearnNode(classifier, model_hypers=node_params, name=f\"{name}-{idx}\")\n",
    "\n",
    "    # Fit the node and make predictions\n",
    "    start = time.time()\n",
    "    print(f\"Fitting {name}\")\n",
    "    node.fit(trained_states, Y_train)\n",
    "\n",
    "    print(f\"Running {name}\")\n",
    "    Y_pred = node.run(tested_states)\n",
    "    runtime = time.time() - start\n",
    "\n",
    "    # Compute metrics for the node\n",
    "    node_metrics = evaluate_performance(Y_pred, Y_test, runtime)\n",
    "    \n",
    "    # Store data for the classifier\n",
    "    results.append({\n",
    "        \"train_states\": trained_states,\n",
    "        \"test_states\": tested_states,\n",
    "        \"Y_test\": Y_test,\n",
    "        \"Y_pred\": Y_pred,\n",
    "        \"metrics\": node_metrics\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061f58c",
   "metadata": {},
   "source": [
    "### 9.1. Compute Final Classification Metrics\n",
    "\n",
    "We compute our final classification metrics by computing the mean of each metric across the cross validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075eab73",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.analysis import log_metrics\n",
    "from utils.analysis import compute_mean_metrics\n",
    "\n",
    "# extract the metrics for each fold\n",
    "fold_metrics = [results[fold]['metrics'] for fold in range(folds)]\n",
    "\n",
    "# compute the mean of the metrics across the folds\n",
    "metrics = compute_mean_metrics(fold_metrics)\n",
    "\n",
    "log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaf1fa",
   "metadata": {},
   "source": [
    "## 10. Analysing Performance Metrics\n",
    "Once classification is complete, we can plot the resulting metrics to understand how our model performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350fcba",
   "metadata": {},
   "source": [
    "### 10.1. Deviation Across Folds\n",
    "We can plot the difference of the metrics per fold, visualising how shuffling the dataset effects the model.\n",
    "    - We should see high-similarity between folds which indicates that our model has not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2107e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_metrics_across_folds\n",
    "\n",
    "plot_metrics_across_folds(fold_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1e756",
   "metadata": {},
   "source": [
    "### 10.2. Class Prediction Metrics\n",
    "Next, we can plot the precision, recall, and F1 score of each class, indicating the model's ability to predict each class individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbad7b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_class_metrics\n",
    "\n",
    "plot_class_metrics(metrics['class_metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4826f0b",
   "metadata": {},
   "source": [
    "### 10.3. Confusion Matrix\n",
    "We can also plot the confusion matrix, which visualises the true labels against the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c494f1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a2ae1",
   "metadata": {},
   "source": [
    "### 10.4. Pediction Clustering\n",
    "Finally, we can plot the t-SNE clustering of our model's predictions which visualises the underlying non-linear relationships between predictions by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0f7fa",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.visualisation import plot_tsne_clustering\n",
    "import numpy as np\n",
    "\n",
    "# concatenate the predictions across the folds\n",
    "Y_pred = []\n",
    "Y_true = []\n",
    "for fold in range(folds):\n",
    "    Y_pred.append(folds_data[fold]['Y_pred'])\n",
    "    Y_true.append(folds_data[fold]['Y_test'])\n",
    "\n",
    "Y_pred = np.concatenate(Y_pred)\n",
    "Y_true = np.concatenate(Y_true)\n",
    "\n",
    "plot_tsne_clustering(Y_pred, Y_true, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
